{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c5fa25b-7052-4e32-9240-b9e18bdd7384",
   "metadata": {},
   "source": [
    "The goal of this first section is to get familiar with the [FastRTC library](https://github.com/gradio-app/fastrtc) ‚Äî the Python library for real-time communication ‚Äî which lets us turn any Python function into a real-time audio and video stream over WebRTC or WebSockets.\n",
    "\n",
    "> We'll use this library to communicate with our agents in real time, just like having a conversation with a colleague!\n",
    "\n",
    "In this first notebook, we'll teach you the basics of FastRTC, starting with very simple handlers (echo handlers), and gradually moving on to handlers that involve calling LLM providers, and finally, ReACT agents with tool use.\n",
    "\n",
    "![FastRTC Logo](img/fastrtc_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4aa79e-44a2-4706-8db5-068f2bce1149",
   "metadata": {},
   "source": [
    "## Understanding FastRTC core concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c9602b-f45a-4b56-b712-66e774054fea",
   "metadata": {},
   "source": [
    "The heart of FastRTC is the powerful [Stream](https://fastrtc.org/reference/stream/) object, which handles real-time streaming of audio, video, or both.\n",
    "\n",
    "üîÑ **Streaming Modes**\n",
    "\n",
    "FastRTC supports three streaming modes, depending on your application's needs:\n",
    "\n",
    "* `send-receive`: Enables full bidirectional communication. This is the mode we‚Äôll use, ideal for conversational agents.\n",
    "* `send`: Streams data from client to server only.\n",
    "* `receive`: Streams data from server to client only.\n",
    "\n",
    "üéôÔ∏è **Modalities**\n",
    "\n",
    "You can build your application around one of three modalities:\n",
    "\n",
    "* `video`: For real-time video streaming.\n",
    "* `audio`: For real-time audio ‚Äî this is our focus, since we‚Äôre building phone-call-style agents.\n",
    "* `audio-video`: Combines both streams for full audiovisual experiences.\n",
    "\n",
    "üß† **Handlers**\n",
    "\n",
    "The handler is the core intelligence behind your `Stream` ‚Äî it's where you define how incoming data should be processed. For audio streams, you'll typically implement a class that inherits from either `StreamHandler` or `AsyncStreamHandler`, depending on whether your processing logic is synchronous or asynchronous.\n",
    "\n",
    "FastRTC also offers a convenient built-in option: the `ReplyOnPause` handler. This handler uses **voice activity detection (VAD)** to determine when the user has finished speaking, and only then sends the collected audio to your generator function. We‚Äôll be using this approach in some of our examples to simplify interaction and improve responsiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627be80-1d83-4b64-b60b-15dc661c56cb",
   "metadata": {},
   "source": [
    "---\n",
    "**‚ö†Ô∏è  IMPORTANT!  ‚ö†Ô∏è**\n",
    "\n",
    "Be sure to set up your `.env` file and install the project first, since we'll need those configuration settings before moving forward.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113a47c-f77b-46b8-95f6-48c444e4085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009adfc3-d5cf-4dd6-98c1-8cdf6be75c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from realtime_phone_agents.config import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87593ae9-2f33-4f94-ae23-adf194ddb684",
   "metadata": {},
   "source": [
    "## Example 1: Echo Audio (StreamHandler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53667e-cef3-4ae6-8251-7c3ab83cf622",
   "metadata": {},
   "source": [
    "`StreamHandler` is a low-level abstraction that gives you full control over how audio is received, processed, and returned ‚Äî both for the input and output streams.\n",
    "\n",
    "In the following example, we'll use it to create a simple `EchoHandler`: the handler will send the user's audio back exactly as it was received."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f9caa-58f3-42f4-b6dc-03ca039db612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "from fastrtc import Stream, StreamHandler\n",
    "\n",
    "\n",
    "class EchoHandler(StreamHandler):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.queue = Queue()\n",
    "\n",
    "    def receive(self, frame: tuple[int, np.ndarray]) -> None:\n",
    "        self.queue.put(frame)\n",
    "\n",
    "    def emit(self) -> None:  #\n",
    "        return self.queue.get()\n",
    "\n",
    "    def copy(self) -> StreamHandler:\n",
    "        return EchoHandler()\n",
    "\n",
    "    def shutdown(self) -> None:  #\n",
    "        pass\n",
    "\n",
    "    def start_up(self) -> None:  #\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88141a5d-06c7-4710-bb8a-295e905bd9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = Stream(handler=EchoHandler(), modality=\"audio\", mode=\"send-receive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99cfea-cbd4-41eb-88f0-c103fa5a26da",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55572397-e102-417d-b2b3-656707f19f7e",
   "metadata": {},
   "source": [
    "We could do exactly the same, but with an `AsyncStreamHandler`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22ad023-c004-47cf-840b-4b06e078f97c",
   "metadata": {},
   "source": [
    "## Example 2: Async Echo Audio (AsyncStreamHandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b67bb21-b449-4d50-94f8-a18306086ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "import numpy as np\n",
    "from fastrtc import AsyncStreamHandler, Stream, wait_for_item\n",
    "\n",
    "\n",
    "class AsyncEchoHandler(AsyncStreamHandler):\n",
    "    \"\"\"Simple Async Echo Handler\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(input_sample_rate=24000)\n",
    "        self.queue = asyncio.Queue()\n",
    "\n",
    "    async def receive(self, frame: tuple[int, np.ndarray]) -> None:\n",
    "        await self.queue.put(frame)\n",
    "\n",
    "    async def emit(self) -> None:\n",
    "        return await wait_for_item(self.queue)\n",
    "\n",
    "    def copy(self):\n",
    "        return AsyncEchoHandler()\n",
    "\n",
    "    async def shutdown(self):\n",
    "        pass\n",
    "\n",
    "    async def start_up(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7666176-c1c9-41a0-b005-a61bfef27382",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = Stream(handler=AsyncEchoHandler(), modality=\"audio\", mode=\"send-receive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d3bdd3-8d8f-476e-b7c0-fba3b4b3698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2774603b-5a44-457c-a0c0-8ebff3b7c0f3",
   "metadata": {},
   "source": [
    "## Example 3: ReplyOnPause Handler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55735076-ea42-49ca-a9ea-89036dc6f993",
   "metadata": {},
   "source": [
    "Luckily for us, FastRTC provides a convenient built-in handler called [ReplyOnPause](https://fastrtc.org/reference/reply_on_pause/) that does exactly what we need for a phone calling experience.This handler listens to incoming audio, waits for the user to pause, and then calls a reply function (fn) when that pause is detected.\n",
    "\n",
    "Here's how it works: it collects audio chunks while the user speaks, uses a **Voice Activity Detection (VAD)** model to detect when speech is happening, and identifies pauses based on configurable thresholds. Once it detects a pause after speech has started, it sends the accumulated audio to your reply function.\n",
    "\n",
    "You can also define an optional `startup_fn` to run when the stream begins, and the handler can interrupt a running reply if new audio arrives ‚Äî making your agent more responsive and natural in conversation.\n",
    "\n",
    "Let's emulate our `EchoAudioHandler` with the `ReplyOnPause` handler!\n",
    "\n",
    "> The experiment is not identical, since our first handlers where returning the frames directly, in this case, the ReplyOnPause is going to return the accumulated audio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9beb2e-9f2c-42fc-ae42-86ad851eb92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastrtc import ReplyOnPause, Stream\n",
    "\n",
    "\n",
    "def echo(audio: tuple[int, np.ndarray]):\n",
    "    yield audio\n",
    "\n",
    "\n",
    "stream = Stream(\n",
    "    handler=ReplyOnPause(echo),\n",
    "    modality=\"audio\",\n",
    "    mode=\"send-receive\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ec94e5-4003-4b61-86c0-1e9d70d89fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b613a-d0ba-4b9f-9e0b-af42c0645db9",
   "metadata": {},
   "source": [
    "## Example 4: Adding TTS and STT Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db392c9b-ea31-4dc2-9e09-8bff52deb3a9",
   "metadata": {},
   "source": [
    "Now that we have a handler that can detect when the user is speaking ‚Äî and more importantly, when they‚Äôve paused ‚Äî we‚Äôre finally ready to do something useful with their voice, not just echo it back.\n",
    "\n",
    "It‚Äôs time to introduce two key components that will bring our conversational agent to life: **speech-to-text (STT)** and **text-to-speech (TTS)** models.\n",
    "\n",
    "* The **STT model** will transcribe the accumulated audio into text, so we can understand what the user said.\n",
    "* The **TTS model** will take a text response and convert it into audio that we can stream back using FastRTC.\n",
    "\n",
    "To get started, we'll try out FastRTC's built-in models:\n",
    "\n",
    "* For STT, we'll use `Moonshine` ‚Äî it‚Äôs lightweight and simple, perfect for our first version.\n",
    "* For TTS, we‚Äôll go with `Kokoro`, which gives us clear and natural-sounding voices.\n",
    "\n",
    "> In future lessons, we'll upgrade these two models with faster-whisper and Orpheus 3B!\n",
    "\n",
    "Let's upgrade our Echo Audio application, by sending back the same audio, but with a different voice (a `kokoro` voice!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ab2c0-d8dd-4e54-804f-5fdaeac3978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastrtc import ReplyOnPause, Stream, get_stt_model, get_tts_model\n",
    "\n",
    "stt_model = get_stt_model()\n",
    "tts_model = get_tts_model()\n",
    "\n",
    "\n",
    "async def echo(audio: tuple[int, np.ndarray]):\n",
    "    transcription = stt_model.stt(audio)\n",
    "    async for audio_chunk in tts_model.stream_tts(transcription):\n",
    "        yield audio_chunk\n",
    "\n",
    "\n",
    "stream = Stream(\n",
    "    handler=ReplyOnPause(echo),\n",
    "    modality=\"audio\",\n",
    "    mode=\"send-receive\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ceea26-c53e-4fee-b964-abdc5c49388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e518c-59ae-41fe-bf6c-0212884fa912",
   "metadata": {},
   "source": [
    "## Example 5: Generating a response with an Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c03f23-e081-4706-b4b6-67bd8b31a099",
   "metadata": {},
   "source": [
    "We can enhance our system by adding an Agent between the **STT model** and the **TTS model**. For this initial Agent, we‚Äôll ignore any use of Tools.\n",
    "\n",
    "To create our Agent, we will use the new `create_agent` method from LangChain, that allows us to create a **ReAct Agent** in just a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df53693-bac2-4d2b-bc57-bbeee41e182c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fastrtc import ReplyOnPause, Stream, get_stt_model, get_tts_model\n",
    "from langchain.agents import create_agent\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Your name is Sarah, a funny voice assistant who loves telling jokes. \n",
    "You are part of a phone conversation, so don't use emojis or asterisks\n",
    "during your responses.\"\"\"\n",
    "\n",
    "stt_model = get_stt_model()\n",
    "llm = ChatGroq(model=settings.groq.model, api_key=settings.groq.api_key)\n",
    "tts_model = get_tts_model()\n",
    "\n",
    "simple_agent = create_agent(\n",
    "    llm, checkpointer=InMemorySaver(), system_prompt=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae220b-b7bc-4be5-8439-e933ef29bd93",
   "metadata": {},
   "source": [
    "The idea is to take Moonshine's transcribed text and pass it directly to our agent. The agent's response will then be streamed using the Kokoro voice. That's precisely what the next cell does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4b5e8-14f5-4e78-b57b-ad05a75fbf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def simple_agent_handler(audio: tuple[int, np.ndarray]):\n",
    "    # 1. Generate the transcription using Moonshine model\n",
    "    transcription = stt_model.stt(audio)\n",
    "\n",
    "    # 2. Use the transcription as user input to our agent, and wait for the response\n",
    "    response = simple_agent.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": transcription}]},\n",
    "        {\"configurable\": {\"thread_id\": \"test\"}},\n",
    "    )\n",
    "\n",
    "    # 3. Stream the audio response using the Kokoro model\n",
    "    async for audio_chunk in tts_model.stream_tts(response[\"messages\"][-1].content):\n",
    "        yield audio_chunk\n",
    "\n",
    "\n",
    "stream = Stream(\n",
    "    handler=ReplyOnPause(simple_agent_handler),\n",
    "    modality=\"audio\",\n",
    "    mode=\"send-receive\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08608d2-56ad-4872-94de-9ef3e7758e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a1c029-1d1f-4108-9a23-67d508e3a531",
   "metadata": {},
   "source": [
    "## Example 6: Adding Tools to our Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32c212a-c766-4881-a27b-f1bd24050bdb",
   "metadata": {},
   "source": [
    "**But what is an Agent without tools?** One of the most important features of our system is the ability to fetch property information from Superlinked using Tools (more on that in Lesson 2!). Simply generating a response won't always be enough‚Äîespecially when the Agent needs to perform a complex search.\n",
    "\n",
    "To handle this, we'll use a common technique found in ChatGPT Voice and many other products. Whenever the Agent needs to search, we'll send a short message back to the user to acknowledge that the system is working. You could even add extra effects‚Äîlike a typing sound‚Äîto make it feel more interactive.\n",
    "\n",
    "The next cell demonstrates how we implement this approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29151c40-92c3-4cef-a091-251c80187677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from fastrtc import ReplyOnPause, Stream, get_stt_model, get_tts_model\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_property_mock_tool(location: str) -> str:\n",
    "    \"\"\"Retrieve real estate details for properties in a given location.\"\"\"\n",
    "    return (\n",
    "        \"I found one apartment in that area. It features 3 rooms, \"\n",
    "        \"2 bathrooms, and a beautifully designed living room.\"\n",
    "    )\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "Your name is Lisa, and you work for The Neural Maze real estate company. \n",
    "Your task is to provide information about specific apartments using the `search_property_mock_tool`.\n",
    "Don't use asterisks or emojis, as you are engaged in a phone call. Just return short and informative responses.\n",
    "\"\"\"\n",
    "\n",
    "stt_model = get_stt_model()\n",
    "llm = ChatGroq(model=settings.groq.model, api_key=settings.groq.api_key)\n",
    "tts_model = get_tts_model()\n",
    "\n",
    "tool_agent = create_agent(\n",
    "    llm,\n",
    "    checkpointer=InMemorySaver(),\n",
    "    system_prompt=system_prompt,\n",
    "    tools=[search_property_mock_tool],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b6502-f134-49bb-8bc4-c0cc0218acfa",
   "metadata": {},
   "source": [
    "Instead of waiting for the full response as we did before, we now listen to the **stream_updates**. This allows us to detect when the agent is about to use a tool and play our ‚Äútrick‚Äù audio, giving the user the feeling of a more natural, real conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278dc5a0-0b3b-4cad-b34b-83d00f65546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pformat\n",
    "\n",
    "for chunk in tool_agent.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Give me cool appartments on San Francisco please\",\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\"configurable\": {\"thread_id\": \"test\"}},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    for step, data in chunk.items():\n",
    "        print(f\"\\n=== Step: {step} ===\")\n",
    "        print(pformat(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4b2f03-94f4-41ad-9f70-3742875ccdbc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "First, we'll create a few helper functions to load a **keyboard sound**. You can swap this out for any sound you prefer, but we'll keep it simple for now. These functions also ensure compatibility with FastRTC, so you don't have to worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c183a5-03b7-46fd-996c-2d02e9ae1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "def load_keyboard_sound(path: str, target_rate: int = 16000, chunk_ms: int = 100):\n",
    "    \"\"\"\n",
    "    Loads an MP3 keyboard sound and returns it as a list of (sample_rate, np.ndarray)\n",
    "    audio chunks, suitable for your existing streaming format.\n",
    "    \"\"\"\n",
    "    audio = (\n",
    "        AudioSegment.from_file(path)\n",
    "        .set_channels(1)  # mono\n",
    "        .set_frame_rate(target_rate)  # resample to 16k\n",
    "    )\n",
    "\n",
    "    # Convert PCM int16 ‚Üí float32 array in [-1, 1]\n",
    "    samples = np.array(audio.get_array_of_samples()).astype(np.float32)\n",
    "    samples /= 32768.0  # normalize from int16\n",
    "\n",
    "    # Split into chunks\n",
    "    samples_per_chunk = int((target_rate * chunk_ms) / 1000)\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(samples), samples_per_chunk):\n",
    "        chunk = samples[i : i + samples_per_chunk]\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "        chunks.append((target_rate, chunk))\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69dc0b-d8d0-44fb-8a73-e92210104cfa",
   "metadata": {},
   "source": [
    "Now, we need to create another method, that generates that streams the **keyboard sound** as we do with the TTS voice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9cd31-d7a6-47cc-8453-2177ffe940ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sound before starting the process\n",
    "KEYBOARD_AUDIO_CHUNKS = load_keyboard_sound(\"sounds/keyboard.mp3\")\n",
    "\n",
    "\n",
    "async def stream_keyboard_sound(max_duration_s: float = 3.0):\n",
    "    \"\"\"\n",
    "    Streams a random keyboard sound effect for at most `max_duration_s` seconds.\n",
    "\n",
    "    Args:\n",
    "        max_duration_s: Maximum duration (in seconds) to stream.\n",
    "    \"\"\"\n",
    "    if max_duration_s <= 0:\n",
    "        return\n",
    "\n",
    "    total_samples = 0\n",
    "    total_samples_allowed = None\n",
    "\n",
    "    for sample_rate, chunk in KEYBOARD_AUDIO_CHUNKS:\n",
    "        # Initialize allowed sample budget once we know the sample rate\n",
    "        if total_samples_allowed is None:\n",
    "            total_samples_allowed = int(max_duration_s * sample_rate)\n",
    "\n",
    "        if total_samples >= total_samples_allowed:\n",
    "            break\n",
    "\n",
    "        remaining_samples = total_samples_allowed - total_samples\n",
    "\n",
    "        # Trim the chunk if it would exceed the allowed duration\n",
    "        if len(chunk) > remaining_samples:\n",
    "            chunk = chunk[:remaining_samples]\n",
    "\n",
    "        if len(chunk) == 0:\n",
    "            break\n",
    "\n",
    "        yield (sample_rate, chunk)\n",
    "        total_samples += len(chunk)\n",
    "\n",
    "        await asyncio.sleep(0)  # allow event loop to breathe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ac6701-3c80-44e0-b85c-457e96a330fa",
   "metadata": {},
   "source": [
    "Finally, let's build the **Tool Agent Handler**. Remember, this handler will:\n",
    "\n",
    "1. Run the Agent in streaming mode and listen for updates.\n",
    "2. When the Agent emits a message containing a `tool_calls` field, it will play a default audio cue (`Let me look for that in the system`) followed by the keyboard typing sound.\n",
    "3. Deliver the final result once the Agent completes its work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8753abb-a1f3-4560-8e95-6f38e7bc4fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def model_has_tool_calls(model_step_data) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: returns True if this 'model' step contains tool_calls.\n",
    "    The exact schema depends on your agent; adjust as needed.\n",
    "    \"\"\"\n",
    "    msgs = None\n",
    "    if isinstance(model_step_data, dict) and \"messages\" in model_step_data:\n",
    "        msgs = model_step_data[\"messages\"]\n",
    "    elif isinstance(model_step_data, list):\n",
    "        msgs = model_step_data\n",
    "    else:\n",
    "        msgs = [model_step_data]\n",
    "\n",
    "    for msg in msgs:\n",
    "        # Attribute-style (e.g. pydantic/BaseModel objects)\n",
    "        tool_calls = getattr(msg, \"tool_calls\", None)\n",
    "        if tool_calls:\n",
    "            return True\n",
    "\n",
    "        # Dict-style\n",
    "        if isinstance(msg, dict):\n",
    "            if msg.get(\"tool_calls\"):\n",
    "                return True\n",
    "\n",
    "        # Sometimes tool calls live in content parts\n",
    "        content = getattr(msg, \"content\", None) or (\n",
    "            msg.get(\"content\") if isinstance(msg, dict) else None\n",
    "        )\n",
    "        if isinstance(content, list):\n",
    "            for part in content:\n",
    "                if isinstance(part, dict) and part.get(\"tool_calls\"):\n",
    "                    return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "async def tool_agent_handler(audio: tuple[int, np.ndarray], *additional_inputs):\n",
    "    _, tool_use_message, keyboard_sound_seconds = additional_inputs\n",
    "\n",
    "    transcription = stt_model.stt(audio)\n",
    "\n",
    "    for chunk in tool_agent.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": transcription}]},\n",
    "        {\"configurable\": {\"thread_id\": \"test\"}},\n",
    "        stream_mode=\"updates\",\n",
    "    ):\n",
    "        for step, data in chunk.items():\n",
    "            if step == \"model\" and model_has_tool_calls(data):\n",
    "                async for audio_chunk in tts_model.stream_tts(tool_use_message):\n",
    "                    yield audio_chunk\n",
    "\n",
    "                async for kb_chunk in stream_keyboard_sound(keyboard_sound_seconds):\n",
    "                    yield kb_chunk\n",
    "\n",
    "                await asyncio.sleep(5)\n",
    "\n",
    "    final_text = data[\"messages\"][0].content\n",
    "\n",
    "    if not final_text:\n",
    "        final_text = \"I'm sorry, I couldn't find anything useful in the system.\"\n",
    "\n",
    "    async for audio_chunk in tts_model.stream_tts(final_text):\n",
    "        yield audio_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daffd2ce-a0a5-4f76-807c-05ec2a1f3344",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = Stream(\n",
    "    handler=ReplyOnPause(tool_agent_handler),\n",
    "    modality=\"audio\",\n",
    "    mode=\"send-receive\",\n",
    "    additional_inputs=[\n",
    "        gr.Text(\"Let me look for that in the system\", label=\"Tool Use Sentence\"),\n",
    "        gr.Number(3.0, label=\"Max Keyboard Sound Duration\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebb1962-8f04-4f75-afb4-ce696f66af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.ui.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
